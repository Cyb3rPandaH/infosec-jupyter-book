{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis with Spark.SQL: Correlating\n",
    "* **Author**: Jose Rodriguez (@Cyb3rPandah)\n",
    "* **Project**: Infosec Jupyter Book\n",
    "* **Public Organization**: [Open Threat Research](https://github.com/OTRF)\n",
    "* **License**: [Creative Commons Attribution-ShareAlike 4.0 International](https://creativecommons.org/licenses/by-sa/4.0/)\n",
    "* **Reference**: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating SQL view from Mordor APT29 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Spark_Data_Analysis\") \\\n",
    "    .config(\"spark.sql.caseSensitive\",\"True\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expose the dataframe as a SQL view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "apt29Json = 'datasets/apt29_evals_day1_manual_2020-05-01225525.json'\n",
    "\n",
    "apt29Df = spark.read.json(apt29Json)\n",
    "\n",
    "apt29Df.createOrReplaceTempView('apt29')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlating data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get new Processes created by an Account that Logged On over the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataframe has 1 records!!\n",
      "+---------------+--------------+-----------------------------------+-------------------------------+---------+\n",
      "|SubjectUserName|TargetUserName|NewProcessName                     |ParentProcessName              |IpAddress|\n",
      "+---------------+--------------+-----------------------------------+-------------------------------+---------+\n",
      "|NASHUA$        |pbeesly       |C:\\Windows\\System32\\wsmprovhost.exe|C:\\Windows\\System32\\svchost.exe|-        |\n",
      "+---------------+--------------+-----------------------------------+-------------------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lateralMovement = spark.sql(\n",
    "'''\n",
    "SELECT b.SubjectUserName, b.TargetUserName, b.NewProcessName, b.ParentProcessName, a.IpAddress\n",
    "FROM apt29 b\n",
    "INNER JOIN(\n",
    "    SELECT TargetLogonId, LogonType, IpAddress\n",
    "    FROM apt29\n",
    "    WHERE lower(Channel) LIKE '%security%'\n",
    "        AND EventID = 4624\n",
    "        AND LogonType = 3\n",
    "    )a\n",
    "ON a.TargetLogonId = b.TargetLogonId\n",
    "WHERE lower(b.Channel) LIKE '%security%'\n",
    "    AND b.EventID = 4688\n",
    "''')\n",
    "\n",
    "print('This dataframe has {} records!!'.format(lateralMovement.count()))\n",
    "lateralMovement.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add context (Parent Process) to Network Connection events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataframe has 1229 records!!\n",
      "+-------------------------+---------------+---------------+-----------------------+\n",
      "|                    Image|       SourceIp|  DestinationIp|            ParentImage|\n",
      "+-------------------------+---------------+---------------+-----------------------+\n",
      "|C:\\Windows\\System32\\dn...|       10.0.0.4|    172.18.39.2|                   null|\n",
      "|C:\\Windows\\ADWS\\Micros...|0:0:0:0:0:0:0:1|0:0:0:0:0:0:0:1|                   null|\n",
      "|C:\\Windows\\System32\\ls...|0:0:0:0:0:0:0:1|0:0:0:0:0:0:0:1|                   null|\n",
      "|C:\\ProgramData\\victim\\...|       10.0.1.4|    192.168.0.5|C:\\Windows\\explorer.exe|\n",
      "|C:\\Windows\\System32\\sv...|       10.0.1.6|       10.0.0.4|                   null|\n",
      "+-------------------------+---------------+---------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parentProcess = spark.sql(\n",
    "'''\n",
    "SELECT b.Image, b.SourceIp, b.DestinationIp, a.ParentImage\n",
    "FROM apt29 b\n",
    "LEFT JOIN(\n",
    "    SELECT ProcessGuid, ParentImage\n",
    "    FROM apt29\n",
    "    WHERE lower(Channel) LIKE '%sysmon%'\n",
    "        AND EventID = 1\n",
    "    )a\n",
    "ON a.ProcessGuid = b.ProcessGuid\n",
    "WHERE lower(b.Channel) LIKE '%sysmon%'\n",
    "    AND b.EventID = 3\n",
    "''')\n",
    "\n",
    "print('This dataframe has {} records!!'.format(parentProcess.count()))\n",
    "parentProcess.show(n = 5, truncate = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add context (Parent Process) to Processes that made a  Network Connection and modified a Registry Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataframe has 3524 records!!\n",
      "-RECORD 0----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " ParentImage   | C:\\Windows\\System32\\control.exe                                                                                                                                                             \n",
      " Image         | C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\powershell.exe                                                                                                                                   \n",
      " SourceIp      | 10.0.1.4                                                                                                                                                                                    \n",
      " DestinationIp | 192.168.0.5                                                                                                                                                                                 \n",
      " TargetObject  | HKLM\\System\\CurrentControlSet\\Services\\bam\\State\\UserSettings\\S-1-5-21-1830255721-3727074217-2423397540-1107\\\\Device\\HarddiskVolume2\\Windows\\System32\\WindowsPowerShell\\v1.0\\powershell.exe \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modifyRegistry = spark.sql(\n",
    "'''\n",
    "SELECT d.ParentImage, c.Image, c.SourceIp, c.DestinationIp, c.TargetObject\n",
    "FROM apt29 d\n",
    "RIGHT JOIN(\n",
    "    SELECT b.ProcessGuid, b.Image, b.SourceIp, b.DestinationIp, a.TargetObject\n",
    "    FROM apt29 b\n",
    "    INNER JOIN(\n",
    "        SELECT ProcessGuid, TargetObject\n",
    "        FROM apt29\n",
    "        WHERE lower(Channel) LIKE '%sysmon%'\n",
    "            AND EventID = 13\n",
    "        )a\n",
    "    ON b.ProcessGuid = a.ProcessGuid\n",
    "    WHERE lower(b.Channel) LIKE '%sysmon%'\n",
    "        AND b.EventID = 3\n",
    ")c\n",
    "ON d.ProcessGuid = c.ProcessGuid\n",
    "WHERE lower(d.Channel) LIKE '%sysmon%'\n",
    "    AND d.EventID = 1\n",
    "''')\n",
    "\n",
    "print('This dataframe has {} records!!'.format(modifyRegistry.count()))\n",
    "modifyRegistry.show(n = 1, vertical = True,truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thank you! I hope you enjoyed it!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark_Python3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}